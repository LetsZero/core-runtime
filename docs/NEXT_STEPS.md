# Zero ‚Äî Next Steps & Architecture Guide

> This document explains how to extend Zero beyond the frozen Core Runtime.

## üèóÔ∏è Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      ZERO COMPILER (Frontend)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Lexer   ‚îÇ ‚Üí  ‚îÇ Parser  ‚îÇ ‚Üí  ‚îÇ   AST    ‚îÇ ‚Üí  ‚îÇ Semantic  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  Analysis ‚îÇ  ‚îÇ
‚îÇ                                                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  High-level: List, Dict, Classes, Exceptions           ‚îÇ        ‚îÇ
‚îÇ                                                        ‚Üì        ‚îÇ
‚îÇ                                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ
‚îÇ                                              ‚îÇ    Zero IR      ‚îÇ‚îÇ
‚îÇ                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CORE RUNTIME (FROZEN)            ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ Tensor ‚îÇ ‚îÇ Scalar ‚îÇ ‚îÇ Struct ‚îÇ ‚îÇ Core Ops        ‚îÇ ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ (matmul, add..) ‚îÇ ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ Control Flow       ‚îÇ ‚îÇ Memory Model             ‚îÇ‚îÇ ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (if, for, while)   ‚îÇ ‚îÇ (alloc, free, sync)      ‚îÇ‚îÇ ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ ‚îÇ         ‚îÇ
‚îÇ                           Reference implementations  ‚Üì ‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                        ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      BACKENDS (Separate Repos)        ‚îÇ         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ         ‚îÇ
‚îÇ  ‚îÇ LLVM        ‚îÇ    ‚îÇ MLIR        ‚îÇ    ‚îÇ CUDA PTX   ‚îÇ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ (CPU opt)   ‚îÇ    ‚îÇ (dialects)  ‚îÇ    ‚îÇ (GPU)      ‚îÇ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îÇ         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ Core Architectural Principles

> **CRITICAL**: Zero IR should be _boringly small_

### The Minimal IR Philosophy

If Zero IR grows too expressive, you're recreating MLIR badly.

**Zero IR should ONLY contain:**

- ‚úÖ Ops (load, store, add, mul, matmul, etc.)
- ‚úÖ Activations (relu, sigmoid, tanh) ‚Äî v1.1
- ‚úÖ Loops (for, while)
- ‚úÖ Memory (alloc, free, copy)
- ‚úÖ Control flow (if, branch, call, return)

> **v1.1 Status**: Core semantic completeness achieved for ML graphs.
> Performance and dtype expansion are deferred to IR/LLVM backends.

**ML semantics belong in:**

- ‚úÖ MLIR dialects (conv2d, attention, layernorm)
- ‚úÖ Standard library (written IN Zero)
- ‚úÖ Compiler sugar (syntactic transformations)

**Keep Zero IR minimal and stable.**

### What Zero Is Building

You are NOT building:

- ‚ùå "A faster Python"
- ‚ùå "Another ML framework"

You ARE building:

> **A language whose lowest layer is ML-native**

This is the right ambition.

---

## üì¶ Repo Structure (Recommended)

```
LetsZero/
‚îú‚îÄ‚îÄ core-runtime/      ‚Üê FROZEN (this repo)
‚îú‚îÄ‚îÄ zero-compiler/     ‚Üê Frontend: Lexer, Parser, AST, Type System
‚îú‚îÄ‚îÄ zero-llvm/         ‚Üê Backend: Zero IR ‚Üí LLVM IR
‚îú‚îÄ‚îÄ zero-mlir/         ‚Üê (Optional) MLIR dialect for ML ops
‚îî‚îÄ‚îÄ zero-stdlib/       ‚Üê Standard library written IN Zero
```

---

## 1Ô∏è‚É£ Connecting LLVM Backend (`zero-llvm`)

### Purpose

Transform Zero IR into optimized LLVM IR for CPU/GPU execution.

### Key Components

```cpp
// zero-llvm/include/codegen.hpp

class LLVMCodegen {
    llvm::LLVMContext ctx;
    llvm::Module* module;
    llvm::IRBuilder<> builder;

public:
    // Lower Zero IR nodes to LLVM IR
    // LLVM backend is "dumb" - it only understands:
    // - pointers, shapes, strides, loops
    // - loads, stores, calls
    // Tensor semantics are already lowered in Zero IR

    llvm::Value* emit_function(const zero::ir::Function& fn);
    llvm::Value* emit_loop(const zero::ir::Loop& loop);
    llvm::Value* emit_load(llvm::Value* ptr, llvm::Value* offset);
    llvm::Value* emit_store(llvm::Value* ptr, llvm::Value* offset, llvm::Value* val);
    llvm::Value* emit_call(const std::string& fn_name, llvm::ArrayRef<llvm::Value*> args);

    // Optimizations
    void run_optimization_passes();  // Loop vectorization, tiling

    // Output
    void emit_object_file(const std::string& path);
};
```

### Optimization Opportunities

| Zero IR Op  | LLVM Optimization                  |
| ----------- | ---------------------------------- |
| MatMul      | Loop tiling, SIMD vectorization    |
| Elementwise | Auto-vectorization (AVX-512)       |
| Reduce      | Parallel reduction patterns        |
| For loops   | Unrolling, polyhedral optimization |

---

## 2Ô∏è‚É£ Connecting MLIR (`zero-mlir`)

### When to Use MLIR

- Custom ML dialects (conv2d, attention, layernorm)
- Multi-level lowering (high-level ‚Üí linalg ‚Üí affine ‚Üí LLVM)
- Better optimization for ML workloads

### Dialect Design

```mlir
// zero-mlir/dialects/ZeroOps.td

def Zero_MatMulOp : Zero_Op<"matmul"> {
  let arguments = (ins AnyTensor:$lhs, AnyTensor:$rhs);
  let results = (outs AnyTensor:$result);
}

// Lowering: Zero.matmul ‚Üí linalg.matmul ‚Üí affine loops ‚Üí LLVM
```

---

## 3Ô∏è‚É£ High-Level Features (List, Dict, Classes)

### The Erasure Principle

> **All high-level constructs are ERASED before reaching the Core Runtime.**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Zero Source                                                   ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                 ‚îÇ
‚îÇ  let x: List[f32] = [1.0, 2.0, 3.0]                           ‚îÇ
‚îÇ  let y = x.map(fn(v) => v * 2)                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ Compiler lowers to:
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Zero IR (what Core Runtime sees)                              ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                             ‚îÇ
‚îÇ  %x = tensor.alloc([3], f32)                                   ‚îÇ
‚îÇ  store %x, [1.0, 2.0, 3.0]                                    ‚îÇ
‚îÇ  %y = tensor.alloc([3], f32)                                   ‚îÇ
‚îÇ  for %i in 0..3:                                               ‚îÇ
‚îÇ      %v = load %x[%i]                                          ‚îÇ
‚îÇ      %r = mul %v, 2.0                                          ‚îÇ
‚îÇ      store %y[%i], %r                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Implementation Strategy

| Feature       | Frontend Representation | Core Runtime Lowering                         |
| ------------- | ----------------------- | --------------------------------------------- |
| `List[T]`     | Dynamic array type      | `Tensor` (1D) + length field                  |
| `Dict[K,V]`   | Hash map type           | `Tensor` (keys) + `Tensor` (values) + hash fn |
| `class Foo`   | Named struct + methods  | `Struct` + standalone functions               |
| `try/catch`   | Exception node          | Control flow with error codes                 |
| `async/await` | Coroutine               | State machine in `Struct`                     |

### Example: List Implementation

```cpp
// In zero-compiler: AST ‚Üí Zero IR lowering

// List[f32] becomes:
struct ZeroList {
    zero::Tensor data;      // Underlying storage
    int64_t length;         // Current length
    int64_t capacity;       // Allocated capacity
};

// list.append(x) becomes:
void list_append(ZeroList* list, float x) {
    if (list->length >= list->capacity) {
        // Grow: allocate new tensor, copy, free old
    }
    store(list->data, list->length, x);
    list->length++;
}
```

### Example: Dict Implementation

```cpp
// Dict[str, f32] becomes:
struct ZeroDict {
    zero::Tensor keys;      // String keys (or hashes)
    zero::Tensor values;    // f32 values
    zero::Tensor occupied;  // Bitmap for slots
    int64_t size;
    int64_t capacity;
};
```

---

## 4Ô∏è‚É£ Development Roadmap

### Phase 1: Zero Compiler MVP

- [ ] Lexer/Tokenizer
- [ ] Parser ‚Üí AST
- [ ] Type checker
- [ ] Zero IR generation
- [ ] Interpreter (for testing)

### Phase 2: LLVM Backend

- [ ] Basic codegen (scalar ops)
- [ ] Tensor allocation/access
- [ ] Control flow
- [ ] Function calls
- [ ] Optimization passes

### Phase 3: High-Level Features (Limit Scope)

> **Focus**: "Enough to write training loops" ‚Äî skip fancy stdlib initially

- [ ] List type + basic operations (append, index, len)
- [ ] Dict type + basic operations (get, set, has)
- [ ] String type (minimal: concat, compare)
- [ ] Error handling (basic: panic, assert)

**What to skip for now:**

- ‚ùå Advanced list methods (sort, filter, etc.)
- ‚ùå Complex string operations (regex, formatting)
- ‚ùå Full exception system with try/catch

### Phase 4A: ML Features (Runtime-Based)

> **CRITICAL**: Don't make autograd a compiler pass first. Many projects die here.

- [ ] Autograd (runtime tape, PyTorch-style)
  - [ ] Tape-based gradient tracking
  - [ ] Backward pass execution
  - [ ] Basic optimizer (SGD)
- [ ] CPU training works end-to-end
- [ ] Model serialization (weights only)

**Why runtime-first?**

- You need working ML examples **fast**
- IR-level AD is hard and thankless early
- Delay elegance. Ship usefulness.

### Phase 4B: Advanced Backends

- [ ] GPU backend (CUDA/PTX)
- [ ] MLIR integration
- [ ] Kernel fusion
- [ ] IR-level autograd (MLIR-friendly)

---

## üîó Key Interfaces

### Zero IR Format (In-Memory C++ Structs)

> **CRITICAL**: Zero IR is NOT JSON-first. JSON is only for debug/inspection.

```cpp
// zero-compiler/include/zero/ir/ir.hpp

namespace zero::ir {

enum class OpKind {
    Load, Store, Add, Mul, MatMul,
    Loop, Branch, Call, Return
};

struct Value {
    std::string name;
    DType dtype;
    std::vector<int64_t> shape;
};

struct Op {
    OpKind kind;
    std::vector<Value*> inputs;
    std::vector<Value*> outputs;
    std::map<std::string, Attribute> attrs;
};

struct BasicBlock {
    std::string name;
    std::vector<Op*> ops;
    BasicBlock* next;
};

struct Function {
    std::string name;
    std::vector<Value*> inputs;
    std::vector<Value*> outputs;
    std::vector<BasicBlock*> blocks;
};

struct Module {
    std::vector<Function*> functions;

    // Binary serialization (flatbuffers/protobuf/custom)
    void serialize_to_binary(const std::string& path);
    static Module* deserialize_from_binary(const std::string& path);

    // JSON = debug dump only
    std::string to_json_debug() const;
};

} // namespace zero::ir
```

**Why not JSON-first?**

- JSON IR kills compile speed
- Makes pattern matching painful
- LLVM/MLIR integrations will hate you
- **Think: JSON is printf for IR, not the IR itself**

### C API for Core Runtime

```c
// For embedding or FFI
extern "C" {
    ZeroTensor* zero_tensor_alloc(int64_t* shape, int ndim, ZeroDType dtype);
    void zero_tensor_free(ZeroTensor* t);
    void zero_matmul(ZeroTensor* a, ZeroTensor* b, ZeroTensor* c);
    // ...
}
```

---

## ‚úÖ Checklist Before Starting New Repo

1. **Clone Core Runtime** ‚Äî Use as submodule or copy headers
2. **Define Zero IR spec** ‚Äî What nodes, what serialization format?
3. **Decide on LLVM version** ‚Äî Currently targeting LLVM 17+
4. **Set up CI** ‚Äî Test on Linux, macOS, Windows

---

## üìù Summary of Key Architectural Decisions

1. **Zero IR is NOT JSON-first** ‚Äî In-memory C++ structs + binary serialization. JSON = debug only.
2. **LLVM backend is "dumb"** ‚Äî Only understands pointers, loops, loads, stores. No Tensor semantics.
3. **Autograd is runtime-first** ‚Äî PyTorch-style tape before IR-level AD.
4. **Zero IR is boringly small** ‚Äî Ops, loops, memory, control flow. That's it.
5. **ML semantics live elsewhere** ‚Äî MLIR dialects, stdlib, compiler sugar.

---

_"The Core Runtime is the law. Everything else is just syntax sugar."_
